{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5RCysOH9jUp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVRV4pYw9jUt"
   },
   "source": [
    "It is highly recommended to read the documentation before implementing any alogorithm.\n",
    "\n",
    "LINEAR REGRESSION : \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html <br>\n",
    "LOGISTIC REGRESSION :\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html<br>\n",
    "SVM REGRESSOR :\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html<br>\n",
    "SVM CLASSIFIER :\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html<br>\n",
    "DECISION TREE CLASSIFIER :\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html<br>\n",
    "DECISION TREE Regressor :\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html<br>\n",
    "RANDOM FOREST CLASSIFIER :\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html<br>\n",
    "RANDOM FOREST CLASSIFIER :\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html<br>\n",
    "\n",
    "METRICS:\n",
    "R2_SCORE : http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html<br>\n",
    "MSE : http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html<br>\n",
    "MAE : http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html<br>\n",
    "Confusion Matrix : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lclLEtp9jUu"
   },
   "source": [
    "WRITE THE CODE FOR IMPORTING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRV20Tv-9jUu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>mileage</th>\n",
       "      <th>color</th>\n",
       "      <th>AGE_IN_YEARS</th>\n",
       "      <th>chevrolet</th>\n",
       "      <th>dodge</th>\n",
       "      <th>gmc</th>\n",
       "      <th>jeep</th>\n",
       "      <th>nissan</th>\n",
       "      <th>...</th>\n",
       "      <th>black</th>\n",
       "      <th>blue</th>\n",
       "      <th>gray</th>\n",
       "      <th>no_color</th>\n",
       "      <th>silver</th>\n",
       "      <th>white</th>\n",
       "      <th>Goa</th>\n",
       "      <th>MP</th>\n",
       "      <th>Maharashtra</th>\n",
       "      <th>UP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2899</td>\n",
       "      <td>2011</td>\n",
       "      <td>190552.0</td>\n",
       "      <td>silver</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5350</td>\n",
       "      <td>2018</td>\n",
       "      <td>39590.0</td>\n",
       "      <td>silver</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25000</td>\n",
       "      <td>2014</td>\n",
       "      <td>64146.0</td>\n",
       "      <td>blue</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27700</td>\n",
       "      <td>2018</td>\n",
       "      <td>6654.0</td>\n",
       "      <td>red</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5700</td>\n",
       "      <td>2018</td>\n",
       "      <td>45561.0</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   mileage   color  AGE_IN_YEARS  chevrolet  dodge  gmc  jeep  \\\n",
       "0   2899  2011  190552.0  silver             3          0      0    0     0   \n",
       "1   5350  2018   39590.0  silver             2          0      1    0     0   \n",
       "2  25000  2014   64146.0    blue             2          0      0    0     0   \n",
       "3  27700  2018    6654.0     red             3          1      0    0     0   \n",
       "4   5700  2018   45561.0   white             1          0      1    0     0   \n",
       "\n",
       "   nissan ...  black  blue  gray  no_color  silver  white  Goa  MP  \\\n",
       "0       0 ...      0     0     0         0       1      0    0   0   \n",
       "1       0 ...      0     0     0         0       1      0    0   1   \n",
       "2       0 ...      0     1     0         0       0      0    0   0   \n",
       "3       0 ...      0     0     0         0       0      0    0   0   \n",
       "4       0 ...      0     0     0         0       0      1    0   0   \n",
       "\n",
       "   Maharashtra  UP  \n",
       "0            0   1  \n",
       "1            0   0  \n",
       "2            0   1  \n",
       "3            1   0  \n",
       "4            0   1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WRITE CODE HERE IMPORT THE DATA AS 'data'\n",
    "data = pd.read_csv('WEEK4.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmude2rPzgyy",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black</th>\n",
       "      <th>blue</th>\n",
       "      <th>gray</th>\n",
       "      <th>no_color</th>\n",
       "      <th>red</th>\n",
       "      <th>silver</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2153 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      black  blue  gray  no_color  red  silver  white\n",
       "0         0     0     0         0    0       1      0\n",
       "1         0     0     0         0    0       1      0\n",
       "2         0     1     0         0    0       0      0\n",
       "3         0     0     0         0    1       0      0\n",
       "4         0     0     0         0    0       0      1\n",
       "5         1     0     0         0    0       0      0\n",
       "6         0     0     1         0    0       0      0\n",
       "7         0     0     0         0    0       1      0\n",
       "8         1     0     0         0    0       0      0\n",
       "9         0     0     0         0    0       0      1\n",
       "10        1     0     0         0    0       0      0\n",
       "11        1     0     0         0    0       0      0\n",
       "12        0     0     0         0    0       0      1\n",
       "13        0     0     0         0    0       0      1\n",
       "14        0     0     1         0    0       0      0\n",
       "15        0     0     0         0    0       0      1\n",
       "16        1     0     0         0    0       0      0\n",
       "17        0     1     0         0    0       0      0\n",
       "18        0     0     0         0    0       1      0\n",
       "19        0     0     0         0    0       1      0\n",
       "20        0     0     0         0    0       0      1\n",
       "21        0     0     0         0    0       0      1\n",
       "22        0     0     0         0    0       1      0\n",
       "23        0     0     0         0    1       0      0\n",
       "24        0     0     0         1    0       0      0\n",
       "25        0     0     1         0    0       0      0\n",
       "26        1     0     0         0    0       0      0\n",
       "27        0     0     0         0    0       0      1\n",
       "28        0     0     0         0    0       0      1\n",
       "29        0     0     0         0    0       0      1\n",
       "...     ...   ...   ...       ...  ...     ...    ...\n",
       "2123      0     0     0         0    0       0      1\n",
       "2124      0     0     0         0    0       1      0\n",
       "2125      0     0     0         0    0       0      1\n",
       "2126      1     0     0         0    0       0      0\n",
       "2127      1     0     0         0    0       0      0\n",
       "2128      1     0     0         0    0       0      0\n",
       "2129      1     0     0         0    0       0      0\n",
       "2130      0     0     0         0    0       0      1\n",
       "2131      1     0     0         0    0       0      0\n",
       "2132      1     0     0         0    0       0      0\n",
       "2133      1     0     0         0    0       0      0\n",
       "2134      0     0     1         0    0       0      0\n",
       "2135      0     0     0         0    0       1      0\n",
       "2136      0     0     1         0    0       0      0\n",
       "2137      1     0     0         0    0       0      0\n",
       "2138      0     0     0         0    0       1      0\n",
       "2139      1     0     0         0    0       0      0\n",
       "2140      1     0     0         0    0       0      0\n",
       "2141      0     0     0         0    0       0      1\n",
       "2142      0     0     0         0    1       0      0\n",
       "2143      0     0     1         0    0       0      0\n",
       "2144      0     1     0         0    0       0      0\n",
       "2145      1     0     0         0    0       0      0\n",
       "2146      1     0     0         0    0       0      0\n",
       "2147      1     0     0         0    0       0      0\n",
       "2148      0     0     0         0    1       0      0\n",
       "2149      0     0     0         0    0       1      0\n",
       "2150      0     0     0         0    0       1      0\n",
       "2151      1     0     0         0    0       0      0\n",
       "2152      0     0     0         0    0       1      0\n",
       "\n",
       "[2153 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(data['color'])#.drop('white',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vFgdtQPl9jUx"
   },
   "source": [
    "ONE HOT ENCODING THE COLOR COLUMN AND THEN DROPPING THE COLOR COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wr-anAzs9jUy"
   },
   "outputs": [],
   "source": [
    "#ONE HOT ENCODING THE COLOR COLUMN\n",
    "data = pd.concat([data,pd.get_dummies(data['color']).drop('white',axis=1)],axis = 1)\n",
    "data = data.drop('color',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dAxfI_kg9jU0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>mileage</th>\n",
       "      <th>AGE_IN_YEARS</th>\n",
       "      <th>chevrolet</th>\n",
       "      <th>dodge</th>\n",
       "      <th>gmc</th>\n",
       "      <th>jeep</th>\n",
       "      <th>nissan</th>\n",
       "      <th>salvage insurance</th>\n",
       "      <th>...</th>\n",
       "      <th>Goa</th>\n",
       "      <th>MP</th>\n",
       "      <th>Maharashtra</th>\n",
       "      <th>UP</th>\n",
       "      <th>black</th>\n",
       "      <th>blue</th>\n",
       "      <th>gray</th>\n",
       "      <th>no_color</th>\n",
       "      <th>red</th>\n",
       "      <th>silver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2899</td>\n",
       "      <td>2011</td>\n",
       "      <td>190552.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5350</td>\n",
       "      <td>2018</td>\n",
       "      <td>39590.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25000</td>\n",
       "      <td>2014</td>\n",
       "      <td>64146.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27700</td>\n",
       "      <td>2018</td>\n",
       "      <td>6654.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5700</td>\n",
       "      <td>2018</td>\n",
       "      <td>45561.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   mileage  AGE_IN_YEARS  chevrolet  dodge  gmc  jeep  nissan  \\\n",
       "0   2899  2011  190552.0             3          0      0    0     0       0   \n",
       "1   5350  2018   39590.0             2          0      1    0     0       0   \n",
       "2  25000  2014   64146.0             2          0      0    0     0       0   \n",
       "3  27700  2018    6654.0             3          1      0    0     0       0   \n",
       "4   5700  2018   45561.0             1          0      1    0     0       0   \n",
       "\n",
       "   salvage insurance   ...    Goa  MP  Maharashtra  UP  black  blue  gray  \\\n",
       "0                  0   ...      0   0            0   1      0     0     0   \n",
       "1                  0   ...      0   1            0   0      0     0     0   \n",
       "2                  0   ...      0   0            0   1      0     1     0   \n",
       "3                  0   ...      0   0            1   0      0     0     0   \n",
       "4                  0   ...      0   0            0   1      0     0     0   \n",
       "\n",
       "   no_color  red  silver  \n",
       "0         0    0       1  \n",
       "1         0    0       1  \n",
       "2         0    0       0  \n",
       "3         0    1       0  \n",
       "4         0    0       0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VIEWING THE DATA ONCE\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZcVR3RC9jU4"
   },
   "source": [
    "WE USE THE train_test_split_function TO SPLIT THE DATA INTO TRAIN AND TEST <br>\n",
    "HERE WE IMPORT IT FROM SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "di5Jh08_9jU4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ll5Q2xCm9jU7"
   },
   "source": [
    "LET US LOOK AT THE PRICES DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZu32pba9jU7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['price']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jn6ZMkZd9jU_"
   },
   "source": [
    "IT IS LEFT-SKEWED, WE HAVE LEARNT HOW TO HANDLE SKEWED DATA<br>\n",
    "We will either use log transform or sqrt transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nC13GB_f9jU_"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#WE HAVE WRITTEN THE CODE FOR PLOTTING THE HISTOGRAM FOR THE LOG-TRANSFORMED PRICE COLUMN AND SQRT TRANSFORMED SQRT COLUMN.\n",
    "plt.hist(np.sqrt(data['price']));\n",
    "plt.title('SQRT TRANSFORM');\n",
    "plt.show()\n",
    "plt.hist(np.log(data['price']));\n",
    "plt.title('LOG TRANSFORM');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIxuDmN89jVC"
   },
   "source": [
    "CLEARLY SQRT TRANSFORM IS BETTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEDTdMPP9jVC"
   },
   "outputs": [],
   "source": [
    "#WE CONVERT PRICES TO THEIR SQRT AND ROUND OFF DECIMALS TO TWO\n",
    "data['price'] = round(np.sqrt(data['price']),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DsRrd_c79jVG"
   },
   "outputs": [],
   "source": [
    "# HERE WE SPLIT DATA INTO TRAIN TEST SPLIT\n",
    "X_train,X_test,y_train,y_test = tts(data.drop('price',axis = 1),data['price'],test_size = 0.2,random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66Pr1G_89jVI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1722, 25)\n",
      "(431, 25)\n",
      "(1722,)\n",
      "(431,)\n",
      "\n",
      "Expected shapes\n",
      "(1722, 25)\n",
      "(431, 25)\n",
      "(1722,)\n",
      "(431,)\n",
      "\n",
      "Verify if they are matching\n"
     ]
    }
   ],
   "source": [
    "#LET US PRINT THE SHAPES\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print('\\nExpected shapes')\n",
    "print((1722, 25))\n",
    "print((431, 25))\n",
    "print((1722,))\n",
    "print((431,))\n",
    "print('\\nVerify if they are matching')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTbjksBn9jVK"
   },
   "source": [
    "NOW <br>\n",
    "LET US APPLY LINEAR REGRESSION ON THE DATA, THIS TIME WE WILL IMPORT IT FROM SKLEARN <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qza5_BMn9jVK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Training R2 score is 0.3577935806672947\n",
      "Linear Regression Testing R2 score is 0.3340597013410347\n",
      "Linear Regression training MEAN SQUARE ERROR is 90997459.24037871\n",
      "Linear Regression Testing MEAN SQUARE ERROR is 93998879.06677724\n",
      "Linear Regression Training mean_absolute_error is 6864.980505017777\n",
      "Linear Regression Testing mean_absolute_error is 6926.987292556904\n",
      "\n",
      "\n",
      "The correct values are :\n",
      "Linear Regression Training R2 score is 0.3577935806672855\n",
      "Linear Regression Testing R2 score is 0.33405970134102436\n",
      "Linear Regression Training mean_square_error is 90997459.24038002\n",
      "Linear Regression Testing mean_square_error is 93998879.06677869\n",
      "Linear Regression Training mean_absolute_error is 6864.980505017769\n",
      "Linear Regression Testing mean_absolute_error is 6926.987292556893\n",
      "\n",
      "Please verify if you have got the same values\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "# INITIALIZE LINEAR REGRESSION WITH ALL DEFAULT PARAMETERS\n",
    "Linreg = LinearRegression()\n",
    "Linreg.fit(X_train,y_train)\n",
    "\n",
    "# FIND TEST ERROR AND TRAIN ERROR WITH THE METRICS IMPORTED ABOVE\n",
    "# NOTE THAT WE HAVE SQUARED BACK THE VALUES AS WE ARE PREDICTING PRICES NOT THEIR SQUARE ROOT \n",
    "print(\"Linear Regression Training R2 score is \" + str(r2_score(np.square(y_train),np.square(Linreg.predict(X_train)))))\n",
    "print(\"Linear Regression Testing R2 score is \" + str(r2_score(np.square(y_test),np.square(Linreg.predict(X_test)))))\n",
    "\n",
    "# WRITE CODE FOR MEAN SQUARE ERROR AND MEAN\n",
    "\n",
    "# Start code\n",
    "print(\"Linear Regression training MEAN SQUARE ERROR is \" + str(mean_squared_error(np.square(y_train),np.square(Linreg.predict(X_train)))))\n",
    "print(\"Linear Regression Testing MEAN SQUARE ERROR is \" + str(mean_squared_error(np.square(y_test),np.square(Linreg.predict(X_test)))))\n",
    "print(\"Linear Regression Training mean_absolute_error is \" + str(mean_absolute_error(np.square(y_train),np.square(Linreg.predict(X_train)))))\n",
    "print(\"Linear Regression Testing mean_absolute_error is \" + str(mean_absolute_error(np.square(y_test),np.square(Linreg.predict(X_test)))))\n",
    "\n",
    "\n",
    "# End code\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"The correct values are :\")\n",
    "\n",
    "\n",
    "print('Linear Regression Training R2 score is 0.3577935806672855')\n",
    "print('Linear Regression Testing R2 score is 0.33405970134102436')\n",
    "print('Linear Regression Training mean_square_error is 90997459.24038002')\n",
    "print('Linear Regression Testing mean_square_error is 93998879.06677869')\n",
    "print('Linear Regression Training mean_absolute_error is 6864.980505017769')\n",
    "print('Linear Regression Testing mean_absolute_error is 6926.987292556893')\n",
    "\n",
    "print(\"\\nPlease verify if you have got the same values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3W4-0--9jVN"
   },
   "source": [
    "<br>\n",
    "NOW LET US WRITE THE SKLEARN IMPLEMENTATION OF DECISION TREE IN REGRESSION <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f43pHl3y9jVO"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# INITIALIZE THE DECISION TREE WITH criterion as mse, max depth as 7 and Random State as 1\n",
    "# USE THE FIT METHOD TO FIND THE R2_SCORE, MSE, MAE For training as well as testing data\n",
    "\n",
    "model= DecisionTreeRegressor(criterion='mse',random_state=1,max_depth=7)\n",
    "model = model.fit(X = X_train,y = y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>58.22</td>\n",
       "      <td>107.200351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>95.39</td>\n",
       "      <td>156.076875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>144.22</td>\n",
       "      <td>131.369070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>46.90</td>\n",
       "      <td>46.091765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>100.50</td>\n",
       "      <td>107.200351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>166.45</td>\n",
       "      <td>164.190421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>127.67</td>\n",
       "      <td>139.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>142.48</td>\n",
       "      <td>152.362190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>127.28</td>\n",
       "      <td>156.076875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>176.07</td>\n",
       "      <td>164.190421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual   Predicted\n",
       "301    58.22  107.200351\n",
       "1828   95.39  156.076875\n",
       "669   144.22  131.369070\n",
       "737    46.90   46.091765\n",
       "2054  100.50  107.200351\n",
       "353   166.45  164.190421\n",
       "708   127.67  139.610000\n",
       "701   142.48  152.362190\n",
       "1282  127.28  156.076875\n",
       "1243  176.07  164.190421"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_Score- 0.5297872769100421\n",
      "Mean Absolute Error: 25.163813620625202\n",
      "Mean Squared Error: 1101.0407611968478\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"R2_Score-\",model.score(X_train,y_train))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Training R2 score is 0.422869230755335\n",
      "Decision Tree Testing R2 score is 0.36145560633416207\n",
      "Decision Tree training MEAN SQUARE ERROR is 81776562.90835412\n",
      "Decision Tree Testing MEAN SQUARE ERROR is 90131889.2396716\n",
      "Decision Tree Training mean_absolute_error is 6266.244817595775\n",
      "Decision Tree Testing mean_absolute_error is 6846.025816839579\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Training R2 score is \" + str(r2_score(np.square(y_train),np.square(model.predict(X_train)))))\n",
    "print(\"Decision Tree Testing R2 score is \" + str(r2_score(np.square(y_test),np.square(model.predict(X_test)))))\n",
    "\n",
    "# WRITE CODE FOR MEAN SQUARE ERROR AND MEAN\n",
    "\n",
    "# Start code\n",
    "print(\"Decision Tree training MEAN SQUARE ERROR is \" + str(mean_squared_error(np.square(y_train),np.square(model.predict(X_train)))))\n",
    "print(\"Decision Tree Testing MEAN SQUARE ERROR is \" + str(mean_squared_error(np.square(y_test),np.square(model.predict(X_test)))))\n",
    "print(\"Decision Tree Training mean_absolute_error is \" + str(mean_absolute_error(np.square(y_train),np.square(model.predict(X_train)))))\n",
    "print(\"Decision Tree Testing mean_absolute_error is \" + str(mean_absolute_error(np.square(y_test),np.square(model.predict(X_test)))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TvQ6cQkC9jVQ"
   },
   "source": [
    "<br>\n",
    "NOW LET US WRITE THE SKLEARN IMPLEMENTATION OF SVM IN REGRESSION <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTlrmqiG9jVQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "# INITIALIZE THE SVM WITH kernel as 'poly', maximum number of iterations as 3000, degree as 2, C(regularization parameter) as 0.1 \n",
    "# FIT THE DATA using FIT METHOD\n",
    "# Calculate the R2_score, mse, mae for training data as well as testing data.\n",
    "\n",
    "# START CODE\n",
    "svr_model = SVR(kernel='poly',max_iter=3000,degree=2, C=0.1)\n",
    "svr_model = svr_model.fit(X_train,y_train)\n",
    "\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Training R2 score is -1.2668220735156703e+30\n",
      "SVR R2 score is -1.2792018213442112e+27\n",
      "SVR training MEAN SQUARE ERROR is 1.7950239444715493e+38\n",
      "SVR Testing MEAN SQUARE ERROR is 1.8056203769117988e+35\n",
      "SVR Training mean_absolute_error is 7.116532563535954e+17\n",
      "SVR Testing mean_absolute_error is 3.513267417707484e+17\n"
     ]
    }
   ],
   "source": [
    "print(\"SVR Training R2 score is \" + str(r2_score(np.square(y_train),np.square(svr_model.predict(X_train)))))\n",
    "print(\"SVR R2 score is \" + str(r2_score(np.square(y_test),np.square(svr_model.predict(X_test)))))\n",
    "print(\"SVR training MEAN SQUARE ERROR is \" + str(mean_squared_error(np.square(y_train),np.square(svr_model.predict(X_train)))))\n",
    "print(\"SVR Testing MEAN SQUARE ERROR is \" + str(mean_squared_error(np.square(y_test),np.square(svr_model.predict(X_test)))))\n",
    "print(\"SVR Training mean_absolute_error is \" + str(mean_absolute_error(np.square(y_train),np.square(svr_model.predict(X_train)))))\n",
    "print(\"SVR Testing mean_absolute_error is \" + str(mean_absolute_error(np.square(y_test),np.square(svr_model.predict(X_test)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GByCRcmJ9jVT"
   },
   "source": [
    "YOU MIGHT SEE WORSE VALUES WHEN COMPARED TO LINEAR REGRESSION OR DECISION TREE AS THE DATA IS UNSCALED AND SVM REQUIRES MUCH MORE PREPROCESSING AND TUNING...BUT WE WON'T DO THAT HERE AS OUR AIM WAS JUST TO IMPLEMENT THE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEE9QB4O9jVT"
   },
   "source": [
    "<br>\n",
    "NOW LET US WRITE THE SKLEARN IMPLEMENTATION OF Random Forest IN REGRESSION <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqrGh5AR9jVU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# INITIALIZE THE Random Forest Regressor WITH no: of trees as 200, max_depth as 9, n_jobs as -1(read documentation for more details) and random state as 1 \n",
    "rf_model = RandomForestRegressor(n_estimators=200,max_depth=9,n_jobs=-1,random_state=1)\n",
    "# FIT THE DATA using FIT METHOD\n",
    "rf_model = rf_model.fit(X_train,y_train)\n",
    "# Calculate the R2_score, mse, mae for training data as well as testing data.\n",
    "\n",
    "# START CODE\n",
    "\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Training R2 score is 0.5524364393379151\n",
      "Random Forest Regressor Testing R2 score is 0.3997413290406362\n",
      "Random Forest Regressor training MEAN SQUARE ERROR is 63417533.12143005\n",
      "Random Forest Regressor Testing MEAN SQUARE ERROR is 84727778.65210526\n",
      "Random Forest Regressor Training mean_absolute_error is 5528.669046457424\n",
      "Random Forest Regressor Testing mean_absolute_error is 6695.668840563837\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Regressor Training R2 score is \" + str(r2_score(np.square(y_train),np.square(rf_model.predict(X_train)))))\n",
    "print(\"Random Forest Regressor Testing R2 score is \" + str(r2_score(np.square(y_test),np.square(rf_model.predict(X_test)))))\n",
    "print(\"Random Forest Regressor training MEAN SQUARE ERROR is \" + str(mean_squared_error(np.square(y_train),np.square(rf_model.predict(X_train)))))\n",
    "print(\"Random Forest Regressor Testing MEAN SQUARE ERROR is \" + str(mean_squared_error(np.square(y_test),np.square(rf_model.predict(X_test)))))\n",
    "print(\"Random Forest Regressor Training mean_absolute_error is \" + str(mean_absolute_error(np.square(y_train),np.square(rf_model.predict(X_train)))))\n",
    "print(\"Random Forest Regressor Testing mean_absolute_error is \" + str(mean_absolute_error(np.square(y_test),np.square(rf_model.predict(X_test)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUEvPws29jVW"
   },
   "source": [
    "NOW FOR CLASSIFICATION MODELS WE NEED TO CONVERT THE TARGET VARIABLE INTO DIFFERENT CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EwOs0dv9jVX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     2153.000000\n",
       "mean     19298.257905\n",
       "std      11904.254199\n",
       "min         25.000000\n",
       "25%      10799.366400\n",
       "50%      17300.140900\n",
       "75%      26001.562500\n",
       "max      74000.320900\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WE FIND THE MEDIAN\n",
    "np.square(data['price']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SxdlNAd49jVa"
   },
   "source": [
    "WE WILL DO BINARY CLASSIFICATION AND DIVIDE THE DATA EQUALLY(ALMOST) INTO 1's AND 0's <br>\n",
    "WE HAVE CHOSEN THRESHOLD AT price = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "waGj0igv9jVa"
   },
   "outputs": [],
   "source": [
    "data.loc[np.square(data['price']) < 20000,'price'] = 0\n",
    "data.loc[np.square(data['price']) >= 20000,'price'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G4pCJ7pw9jVc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>mileage</th>\n",
       "      <th>AGE_IN_YEARS</th>\n",
       "      <th>chevrolet</th>\n",
       "      <th>dodge</th>\n",
       "      <th>gmc</th>\n",
       "      <th>jeep</th>\n",
       "      <th>nissan</th>\n",
       "      <th>salvage insurance</th>\n",
       "      <th>...</th>\n",
       "      <th>Goa</th>\n",
       "      <th>MP</th>\n",
       "      <th>Maharashtra</th>\n",
       "      <th>UP</th>\n",
       "      <th>black</th>\n",
       "      <th>blue</th>\n",
       "      <th>gray</th>\n",
       "      <th>no_color</th>\n",
       "      <th>red</th>\n",
       "      <th>silver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>190552.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>39590.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>64146.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>6654.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>45561.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   mileage  AGE_IN_YEARS  chevrolet  dodge  gmc  jeep  nissan  \\\n",
       "0    0.0  2011  190552.0             3          0      0    0     0       0   \n",
       "1    0.0  2018   39590.0             2          0      1    0     0       0   \n",
       "2    1.0  2014   64146.0             2          0      0    0     0       0   \n",
       "3    1.0  2018    6654.0             3          1      0    0     0       0   \n",
       "4    0.0  2018   45561.0             1          0      1    0     0       0   \n",
       "\n",
       "   salvage insurance   ...    Goa  MP  Maharashtra  UP  black  blue  gray  \\\n",
       "0                  0   ...      0   0            0   1      0     0     0   \n",
       "1                  0   ...      0   1            0   0      0     0     0   \n",
       "2                  0   ...      0   0            0   1      0     1     0   \n",
       "3                  0   ...      0   0            1   0      0     0     0   \n",
       "4                  0   ...      0   0            0   1      0     0     0   \n",
       "\n",
       "   no_color  red  silver  \n",
       "0         0    0       1  \n",
       "1         0    0       1  \n",
       "2         0    0       0  \n",
       "3         0    1       0  \n",
       "4         0    0       0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "no82fZ4Y9jVf"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = tts(data.drop('price',axis = 1),data['price'],test_size = 0.2,random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWKZZWms9jVh"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mWkYGoJZ9jVj"
   },
   "outputs": [],
   "source": [
    "# INITIALIZE Logistic Regression BY TAKING solver as 'lbfgs', max_iter as 2000, C as 0.5 and penalty as 'l2' and random_state as 1 \n",
    "# FIT Logistic Regression ON TRAINING DATA\n",
    "# WRITE CODE HERE AND FIND THE TPR, FPR, SENSITIVITY, SPECIFICITY, ACCURACY FOR LOGISTIC REGRESSION\n",
    "# USE THE IMPORTED CONFUSION MATRIX\n",
    "\n",
    "# START CODE\n",
    "lr_model = LogisticRegression(solver='lbfgs',max_iter=2000,C=0.5,penalty='l2',random_state=1)\n",
    "lr_model= lr_model.fit(X_train, y_train)\n",
    "y_pred_train=lr_model.predict(X_train)\n",
    "y_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "\n",
    "#print( 'Training Accuracy;',lr_model.score(X_train, y_train))\n",
    "#print('Testing accuracy:',accuracy_score(y_test, y_pred))\n",
    "      \n",
    "\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig Confusion matrix= [[782 231]\n",
      " [248 461]]\n",
      "Testing Confusion matrix= [[188  55]\n",
      " [ 65 123]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# creating confusion matrix\n",
    "cm_train = metrics.confusion_matrix(y_train,y_pred_train)\n",
    "cm_test = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "print('Trainig Confusion matrix=',cm_train)\n",
    "print('Testing Confusion matrix=',cm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Negative</th>\n",
       "      <td>782</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Positive</th>\n",
       "      <td>248</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted Negative  Predicted Positive\n",
       "Actual Negative                 782                 231\n",
       "Actual Positive                 248                 461"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning columns names\n",
    "cm_df = pd.DataFrame(cm_train, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE AND FIND THE TPR, FPR, SENSITIVITY, SPECIFICITY, ACCURACY FOR LOGISTIC REGRESSION\n",
    "#True Positive Rate = True Positives / (True Positives + False Negatives)\n",
    "#False Positive Rate = False Positives / (False Positives + True Negatives)\n",
    "#Sensitivity = True Positives / (True Positives + False Negatives)\n",
    "#Specificity = True Negatives / (True Negatives + False Positives)\n",
    "#False Positive Rate = 1 - Specificity\n",
    "\n",
    "def confusion_metrics(conf_matrix):\n",
    "# save confusion matrix and slice into four pieces\n",
    "    TP = conf_matrix[1][1]\n",
    "    TN = conf_matrix[0][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    FN = conf_matrix[1][0]\n",
    "    print('True Positives:', TP)\n",
    "    print('True Negatives:', TN)\n",
    "    print('False Positives:', FP)\n",
    "    print('False Negatives:', FN)\n",
    "    \n",
    "    \n",
    "    # calculate accuracy\n",
    "    conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "    \n",
    "    # calculate mis-classification\n",
    "    conf_misclassification = 1- conf_accuracy\n",
    "    \n",
    "    # calculate the sensitivity\n",
    "    conf_sensitivity = (TP / float(TP + FN))\n",
    "    # calculate the specificity\n",
    "    conf_specificity = (TN / float(TN + FP))\n",
    "    \n",
    "    # calculate precision\n",
    "    conf_precision = (TN / float(TN + FP))\n",
    "    # calculate f_1 score\n",
    "    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))\n",
    "    print('-'*50)\n",
    "    print(f'Accuracy: {round(conf_accuracy,2)}') \n",
    "    print(f'Mis-Classification: {round(conf_misclassification,2)}') \n",
    "    print('Sensitivity or TPR', conf_sensitivity)\n",
    "    print(f'Sensitivity: {round(conf_sensitivity,2)}') \n",
    "    print('Specificity or TNR',conf_specificity )\n",
    "    print(f'Specificity: {round(conf_specificity,2)}') \n",
    "    print(f'Precision: {round(conf_precision,2)}')\n",
    "    print(f'f_1 Score: {round(conf_f1,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training:\n",
      "True Positives: 461\n",
      "True Negatives: 782\n",
      "False Positives: 231\n",
      "False Negatives: 248\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.72\n",
      "Mis-Classification: 0.28\n",
      "Sensitivity or TPR 0.6502115655853314\n",
      "Sensitivity: 0.65\n",
      "Specificity or TNR 0.771964461994077\n",
      "Specificity: 0.77\n",
      "Precision: 0.77\n",
      "f_1 Score: 0.71\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "For testing:\n",
      "True Positives: 123\n",
      "True Negatives: 188\n",
      "False Positives: 55\n",
      "False Negatives: 65\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.72\n",
      "Mis-Classification: 0.28\n",
      "Sensitivity or TPR 0.6542553191489362\n",
      "Sensitivity: 0.65\n",
      "Specificity or TNR 0.7736625514403292\n",
      "Specificity: 0.77\n",
      "Precision: 0.77\n",
      "f_1 Score: 0.71\n"
     ]
    }
   ],
   "source": [
    "print(\"For training:\")\n",
    "confusion_metrics(cm_train)\n",
    "print('+'*50)\n",
    "print(\"For testing:\")\n",
    "confusion_metrics(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260     0.0\n",
       "1769    1.0\n",
       "158     0.0\n",
       "680     1.0\n",
       "1240    1.0\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wGv7G_Q29jVl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=2e-06, kernel='rbf',\n",
       "  max_iter=1000, probability=False, random_state=1, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INITIALIZE SVM BY TAKING kernal as 'rbf', max_iter as 1000 and random_state as 1 and gamma as scale\n",
    "# FIT SVM ON TRAINING DATA\n",
    "# WRITE CODE HERE AND FIND THE TPR, FPR, SENSTIVITY, SPECIFICITY, ACCURACY FOR SVM\n",
    "# USE THE IMPORTED CONFUSION MATRIX\n",
    "\n",
    "svc_model = SVC(kernel='rbf',max_iter=1000,gamma=0.000001, random_state=1)\n",
    "svc_model.fit(X_train,y_train)\n",
    "#plt.scatter()\n",
    "svc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train=svc_model.predict(X_train)\n",
    "y_pred_test = svc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion matrix= [[830 183]\n",
      " [279 430]]\n",
      "Testing Confusion matrix= [[195  48]\n",
      " [ 73 115]]\n"
     ]
    }
   ],
   "source": [
    "# creating confusion matrix\n",
    "cm_train = metrics.confusion_matrix(y_train,y_pred_train)\n",
    "cm_test = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "print('Training Confusion matrix=',cm_train)\n",
    "print('Testing Confusion matrix=',cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_metrics(conf_matrix):\n",
    "# save confusion matrix and slice into four pieces\n",
    "    TP = conf_matrix[1][1]\n",
    "    TN = conf_matrix[0][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    FN = conf_matrix[1][0]\n",
    "    print('True Positives:', TP)\n",
    "    print('True Negatives:', TN)\n",
    "    print('False Positives:', FP)\n",
    "    print('False Negatives:', FN)\n",
    "    \n",
    "    \n",
    "    # calculate accuracy\n",
    "    conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "    \n",
    "    # calculate mis-classification\n",
    "    conf_misclassification = 1- conf_accuracy\n",
    "    \n",
    "    # calculate the sensitivity\n",
    "    conf_sensitivity = (TP / float(TP + FN))\n",
    "    # calculate the specificity\n",
    "    conf_specificity = (TN / float(TN + FP))\n",
    "    \n",
    "    # calculate precision\n",
    "    conf_precision = (TN / float(TN + FP))\n",
    "    # calculate f_1 score\n",
    "    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))\n",
    "    print('-'*50)\n",
    "    print('Accuracy:',conf_accuracy) \n",
    "    print(f'Mis-Classification: {round(conf_misclassification,2)}') \n",
    "    print('Sensitivity or TPR', conf_sensitivity)\n",
    "    print(f'Sensitivity: {round(conf_sensitivity,2)}') \n",
    "    print('Specificity or TNR',conf_specificity )\n",
    "    print(f'Specificity: {round(conf_specificity,2)}') \n",
    "    print(f'Precision: {round(conf_precision,2)}')\n",
    "    print(f'f_1 Score: {round(conf_f1,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training:\n",
      "True Positives: 430\n",
      "True Negatives: 830\n",
      "False Positives: 183\n",
      "False Negatives: 279\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.7317073170731707\n",
      "Mis-Classification: 0.27\n",
      "Sensitivity or TPR 0.6064880112834978\n",
      "Sensitivity: 0.61\n",
      "Specificity or TNR 0.8193484698914116\n",
      "Specificity: 0.82\n",
      "Precision: 0.82\n",
      "f_1 Score: 0.7\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "For testing:\n",
      "True Positives: 115\n",
      "True Negatives: 195\n",
      "False Positives: 48\n",
      "False Negatives: 73\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.7192575406032483\n",
      "Mis-Classification: 0.28\n",
      "Sensitivity or TPR 0.6117021276595744\n",
      "Sensitivity: 0.61\n",
      "Specificity or TNR 0.8024691358024691\n",
      "Specificity: 0.8\n",
      "Precision: 0.8\n",
      "f_1 Score: 0.69\n"
     ]
    }
   ],
   "source": [
    "print(\"For training:\")\n",
    "confusion_metrics(cm_train)\n",
    "print('+'*50)\n",
    "print(\"For testing:\")\n",
    "confusion_metrics(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8OSZT7q9jVn"
   },
   "outputs": [],
   "source": [
    "\n",
    "# INITIALIZE DECISION TREE CLASSIFIER BY TAKING criterion as 'gini' , max_depth as 3, and random_state as 1\n",
    "# FIT Decision Tree ON TRAINING DATA\n",
    "# WRITE CODE HERE AND FIND THE TPR, FPR, SENSTIVITY, SPECIFICITY, ACCURACY FOR Decision Tree\n",
    "# USE THE IMPORTED CONFUSION MATRIX\n",
    "\n",
    "# START CODE\n",
    "dtc_model= DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=1)\n",
    "dtc_model = dtc_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train=dtc_model.predict(X_train)\n",
    "y_pred_test = dtc_model.predict(X_test)\n",
    "\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion matrix= [[889 124]\n",
      " [324 385]]\n",
      "Testing Confusion matrix= [[215  28]\n",
      " [ 82 106]]\n"
     ]
    }
   ],
   "source": [
    "# creating confusion matrix\n",
    "cm_train = metrics.confusion_matrix(y_train,y_pred_train)\n",
    "cm_test = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "print('Training Confusion matrix=',cm_train)\n",
    "print('Testing Confusion matrix=',cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_metrics(conf_matrix):\n",
    "# save confusion matrix and slice into four pieces\n",
    "    TP = conf_matrix[1][1]\n",
    "    TN = conf_matrix[0][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    FN = conf_matrix[1][0]\n",
    "    print('True Positives:', TP)\n",
    "    print('True Negatives:', TN)\n",
    "    print('False Positives:', FP)\n",
    "    print('False Negatives:', FN)\n",
    "    \n",
    "    \n",
    "    # calculate accuracy\n",
    "    conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "    \n",
    "    # calculate mis-classification\n",
    "    conf_misclassification = 1- conf_accuracy\n",
    "    \n",
    "    # calculate the sensitivity\n",
    "    conf_sensitivity = (TP / float(TP + FN))\n",
    "    # calculate the specificity\n",
    "    conf_specificity = (TN / float(TN + FP))\n",
    "    \n",
    "    # calculate precision\n",
    "    conf_precision = (TN / float(TN + FP))\n",
    "    # calculate f_1 score\n",
    "    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))\n",
    "    print('-'*50)\n",
    "    print('Accuracy not roundoff',conf_accuracy)\n",
    "    print(f'Accuracy: {round(conf_accuracy,2)}') \n",
    "    print(f'Mis-Classification: {round(conf_misclassification,2)}') \n",
    "    print('Sensitivity or TPR', conf_sensitivity)\n",
    "    print(f'Sensitivity: {round(conf_sensitivity,2)}') \n",
    "    print('Specificity or TNR',conf_specificity )\n",
    "    print(f'Specificity: {round(conf_specificity,2)}') \n",
    "    print(f'Precision: {round(conf_precision,2)}')\n",
    "    print(f'f_1 Score: {round(conf_f1,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training:\n",
      "True Positives: 385\n",
      "True Negatives: 889\n",
      "False Positives: 124\n",
      "False Negatives: 324\n",
      "--------------------------------------------------\n",
      "Accuracy not roundoff 0.7398373983739838\n",
      "Accuracy: 0.74\n",
      "Mis-Classification: 0.26\n",
      "Sensitivity or TPR 0.5430183356840621\n",
      "Sensitivity: 0.54\n",
      "Specificity or TNR 0.8775913129318855\n",
      "Specificity: 0.88\n",
      "Precision: 0.88\n",
      "f_1 Score: 0.67\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "For testing:\n",
      "True Positives: 106\n",
      "True Negatives: 215\n",
      "False Positives: 28\n",
      "False Negatives: 82\n",
      "--------------------------------------------------\n",
      "Accuracy not roundoff 0.7447795823665894\n",
      "Accuracy: 0.74\n",
      "Mis-Classification: 0.26\n",
      "Sensitivity or TPR 0.5638297872340425\n",
      "Sensitivity: 0.56\n",
      "Specificity or TNR 0.8847736625514403\n",
      "Specificity: 0.88\n",
      "Precision: 0.88\n",
      "f_1 Score: 0.69\n"
     ]
    }
   ],
   "source": [
    "print(\"For training:\")\n",
    "confusion_metrics(cm_train)\n",
    "print('+'*50)\n",
    "print(\"For testing:\")\n",
    "confusion_metrics(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZRx_Rez9jVq"
   },
   "outputs": [],
   "source": [
    "# INITIALIZE RANDOM FOREST CLASSIFIER BY TAKING no:of trees as 100 , max_depth as 3,criterion as 'gini' and random_state as 1 \n",
    "# FIT RANDOM FOREST ON TRAINING DATA\n",
    "# WRITE CODE HERE AND FIND THE TPR, FPR, SENSTIVITY, SPECIFICITY, ACCURACY FOR Random Forest\n",
    "# USE THE IMPORTED CONFUSION MATRIX\n",
    "\n",
    "# START CODE\n",
    "rfc_model = RandomForestClassifier(n_estimators=100,max_depth=3,criterion='gini',random_state=1)\n",
    "rfc_model = rfc_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train=rfc_model.predict(X_train)\n",
    "y_pred_test = rfc_model.predict(X_test)\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion matrix= [[891 122]\n",
      " [323 386]]\n",
      "Testing Confusion matrix= [[214  29]\n",
      " [ 82 106]]\n"
     ]
    }
   ],
   "source": [
    "# creating confusion matrix\n",
    "cm_train = metrics.confusion_matrix(y_train,y_pred_train)\n",
    "cm_test = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "print('Training Confusion matrix=',cm_train)\n",
    "print('Testing Confusion matrix=',cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_metrics(conf_matrix):\n",
    "# save confusion matrix and slice into four pieces\n",
    "    TP = conf_matrix[1][1]\n",
    "    TN = conf_matrix[0][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    FN = conf_matrix[1][0]\n",
    "    print('True Positives:', TP)\n",
    "    print('True Negatives:', TN)\n",
    "    print('False Positives:', FP)\n",
    "    print('False Negatives:', FN)\n",
    "    \n",
    "    \n",
    "    # calculate accuracy\n",
    "    conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "    \n",
    "    # calculate mis-classification\n",
    "    conf_misclassification = 1- conf_accuracy\n",
    "    \n",
    "    # calculate the sensitivity\n",
    "    conf_sensitivity = (TP / float(TP + FN))\n",
    "    # calculate the specificity\n",
    "    conf_specificity = (TN / float(TN + FP))\n",
    "    \n",
    "    # calculate precision\n",
    "    conf_precision = (TN / float(TN + FP))\n",
    "    # calculate f_1 score\n",
    "    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))\n",
    "    print('-'*50)\n",
    "    print('Accuracy not roundoff',conf_accuracy)\n",
    "    print(f'Accuracy: {round(conf_accuracy,2)}') \n",
    "    print(f'Mis-Classification: {round(conf_misclassification,2)}') \n",
    "    print('Sensitivity or TPR', conf_sensitivity)\n",
    "    print(f'Sensitivity: {round(conf_sensitivity,2)}') \n",
    "    print('Specificity or TNR',conf_specificity )\n",
    "    print(f'Specificity: {round(conf_specificity,2)}') \n",
    "    print(f'Precision: {round(conf_precision,2)}')\n",
    "    print(f'f_1 Score: {round(conf_f1,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training:\n",
      "True Positives: 386\n",
      "True Negatives: 891\n",
      "False Positives: 122\n",
      "False Negatives: 323\n",
      "--------------------------------------------------\n",
      "Accuracy not roundoff 0.7415795586527294\n",
      "Accuracy: 0.74\n",
      "Mis-Classification: 0.26\n",
      "Sensitivity or TPR 0.5444287729196051\n",
      "Sensitivity: 0.54\n",
      "Specificity or TNR 0.8795656465942744\n",
      "Specificity: 0.88\n",
      "Precision: 0.88\n",
      "f_1 Score: 0.67\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "For testing:\n",
      "True Positives: 106\n",
      "True Negatives: 214\n",
      "False Positives: 29\n",
      "False Negatives: 82\n",
      "--------------------------------------------------\n",
      "Accuracy not roundoff 0.7424593967517401\n",
      "Accuracy: 0.74\n",
      "Mis-Classification: 0.26\n",
      "Sensitivity or TPR 0.5638297872340425\n",
      "Sensitivity: 0.56\n",
      "Specificity or TNR 0.8806584362139918\n",
      "Specificity: 0.88\n",
      "Precision: 0.88\n",
      "f_1 Score: 0.69\n"
     ]
    }
   ],
   "source": [
    "print(\"For training:\")\n",
    "confusion_metrics(cm_train)\n",
    "print('+'*50)\n",
    "print(\"For testing:\")\n",
    "confusion_metrics(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WEEK4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
